{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT 4**"
      ],
      "metadata": {
        "id": "Pbcc9QS1tnBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Group Name: \\\\\n",
        "   Member Names: \\\\\n",
        "   Member Student Numbers: \\\\\n",
        "   Report Title: \\\\"
      ],
      "metadata": {
        "id": "f2Eeke4Z_EkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derived Datasets**"
      ],
      "metadata": {
        "id": "RMUnCICdyBbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so3pwbPKTDX"
      },
      "source": [
        "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's start by installing spaCy\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "bhFvS3q7Lu0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCWgl6PLKTDY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
      ],
      "metadata": {
        "id": "EX9WQWGSwU2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "add description of the dataset and your justification of the choices made to obtain the derived datasets"
      ],
      "metadata": {
        "id": "CSyg0jjC1jJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xx4qMCLKTDb"
      },
      "outputs": [],
      "source": [
        "#Load the dataset you chose.\n",
        "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
        "url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_AirPassengerReviews.csv'\n",
        "\n",
        "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(url)\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "wg24OUV81Xgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ5nSY1HKTDd"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pycllPKTDd"
      },
      "source": [
        "This is where you create the NLP pipeline. load() will download the correct model (English)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQtSi8XuKTDe"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqFArDyKTDf"
      },
      "source": [
        "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
        "\n",
        "Doc: https://spacy.io/api/doc\n",
        "\n",
        "Token: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcaeMUL2KTDg"
      },
      "outputs": [],
      "source": [
        "#Apply nlp pipeline to the column that has your sentences.\n",
        "data['tokenized'] = data['??'].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i6ai1I8KTDg"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRfZ2uEKTDh"
      },
      "source": [
        "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
        "\n",
        "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create empty dataframes that will store your derived datasets\n",
        "\n",
        "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
        "derived_dataset2 = pd.DataFrame(columns = ['Class', 'pos-np'])"
      ],
      "metadata": {
        "id": "qw0a_2ySyUo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yeak1tAOKTDi"
      },
      "outputs": [],
      "source": [
        "def get_pos(sentence, wanted_pos): #wanted_pos refers to the desired pos tagging\n",
        "    verbs = []\n",
        "    for token in sentence:\n",
        "        if token.pos_ in wanted_pos:\n",
        "            verbs.append(token.lemma_) # lemma returns a number. lemma_ return a string\n",
        "    return ' '.join(verbs) # return value is as a string and not a list for countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147NRzwKKTDj"
      },
      "outputs": [],
      "source": [
        "#As an example, we use the above function to fetch all the verbs. We store this information in our first derived dataset\n",
        "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['VERB']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_bUg_fVKTDk"
      },
      "outputs": [],
      "source": [
        "derived_dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuGv-NnfKTDj"
      },
      "outputs": [],
      "source": [
        "#Change this line to fetch your desired pos taggings for the second derived dataset\n",
        "derived_dataset2['pos-np'] = data['tokenized'].apply(??)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Derived Dataset 2, you also need to include Named Entities\n",
        "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
        "#on the dataset of your choice.\n",
        "#You can choose the types of entities (dates, organization, people) that you want,\n",
        "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
        "#in a previous cell.\n",
        "\n",
        "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "NR7AdW0MfXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pX4RgKhKTDk"
      },
      "source": [
        "Now that you have your derived datasets, you can move to perform your classificaton task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Classification Task**"
      ],
      "metadata": {
        "id": "1GhniwHtzfQt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}